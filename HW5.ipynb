{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "7fbe004d-23a3-4858-a542-5280b5ad3732",
      "cell_type": "markdown",
      "source": "# # Week 5 Homework\n# \nIn this notebook, I provide my solutions for three questions in Week 5 Homework:\n\n- **Q1: PyMC Example Gallery Documentation**  \n  I create a formatted Markdown listing of the contents of the \"PyMC Example Gallery\". This listing includes clickable titles (with links) and displays my favorite image from each page in a table format.\n\n- **Q2: Bayesian Inference using PyMC**  \n  I use PyMC to provide Bayesian inference for the parameters associated with a sample of normal data. I perform three analyses:\n  1. With a *normal prior* for \\(\\theta\\) and a *gamma prior* for \\(\\tau\\).\n  2. With a *non‐normal prior* for \\(\\theta\\) (Laplace) and a *non‐gamma prior* for \\(\\tau\\) (LogNormal).\n  3. With a *Student-t prior* for \\(\\theta\\) and an *Exponential prior* for \\(\\tau\\).  \n  For each model I include diagnostic assessments (summary statistics and trace plots) using ArviZ.\n\n- **Q3: Slice Sampling within Gibbs**  \n  I provide a 、explanation of the slice sampling algorithm and discuss how it can be used in place of a Metropolis–Hastings step in a Metropolis‐within‐Gibbs algorithm when full conditionals are known only up to a normalizing constant. I also include a code demonstration of slice sampling.",
      "metadata": {}
    },
    {
      "id": "c45b7d81-ef01-4996-aa70-a0afcd4f03f4",
      "cell_type": "markdown",
      "source": "# ## Q1: Questions about PyMC",
      "metadata": {}
    },
    {
      "id": "fe5d3dbb-61c2-4a1f-a086-09ec866a441e",
      "cell_type": "markdown",
      "source": "#### How to\n\n- [1. Prior and Posterior Predictive Checks](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html#posterior-predictive)\n- [2. Model comparison](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/model_comparison.html#model-comparison)\n- [3. Updating Priors](https://www.pymc.io/projects/examples/en/latest/howto/updating_priors.html)\n- [4. Automatic marginalization of discrete variables](https://www.pymc.io/projects/examples/en/latest/howto/marginalizing-models.html)\n- [5. How to debug a model](http://pymc.io/projects/examples/en/latest/howto/howto_debugging.html)\n- [6. How to wrap a JAX function for use in PyMC](https://www.pymc.io/projects/examples/en/latest/howto/wrapping_jax_function.html)\n- [7. Splines](https://www.pymc.io/projects/examples/en/latest/howto/spline.html)\n- [8. Bayesian copula estimation: Describing correlated joint distributions](https://www.pymc.io/projects/examples/en/latest/howto/copula-estimation.html)\n- [9. Using ModelBuilder class for deploying PyMC models](https://www.pymc.io/projects/examples/en/latest/howto/model_builder.html)\n- [10. Using a “black box” likelihood function](https://www.pymc.io/projects/examples/en/latest/howto/blackbox_external_likelihood_numpy.html)\n- [11. LKJ Cholesky Covariance Priors for Multivariate Normal Models](https://www.pymc.io/projects/examples/en/latest/howto/LKJ.html)\n- [12. Bayesian Missing Data Imputation](https://www.pymc.io/projects/examples/en/latest/howto/Missing_Data_Imputation.html)\n- [13. Profiling](https://www.pymc.io/projects/examples/en/latest/howto/profiling.html)\n\n| 1 | 2 | 3 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/docs/en/stable/_images/ff23516dbee6a363a9666322f96566d04330e625e30db70966f6d9dd677d6f8d.png) | ![](https://www.pymc.io/projects/docs/en/stable/_images/220695e8447f0651dc133a3389ef5d6f586b78e08421f0a2371928118cc909f1.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/5c91b2b45e4e07be744bd90c92793f4b064b85ea9e147b0eaf3b463e1735f34a.png) |\n\n| 4 | 5 | 6 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/36a3357f89d21ad98e89a8eef3179f83d6897f347714629d8ac8a85bd7100ad3.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/cd8d49dd3519e25c0ec2d3ef30a0816fe9c3f7708d9eda15d157590da6ca27b5.svg) | ![](https://www.pymc.io/projects/examples/en/latest/_images/810705521c6d764cc52621beb88fb0f1160e1a92c804d6708c7c4f84aa34a535.png) |\n\n| 7 | 8 | 9 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/fc05f4904f9bc747ae16a7c7037f87b7dc39a480a2a24f0cff56f2ab9f1b9470.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/copula_schematic.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/bbc4da85106ec9c236813f170272c9b249e0a8af0911c428cddbf54bb0752c74.png) |\n\n| 10 | 11 | 12 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/59d214aaf3f0f298398c687f120f355fb026e96b7feac268a87b9548dbdf05d0.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/f859e4ca959a1c0fc8f886a6752ef8317cff5016e94dc17a7c1cc6ac2cd9535f.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/c9989dd8511d1b594a2a82b778f8eea3617da7df1135c6bd3dd174a50acb4e2e.png) |\n\n| 13 |   |   |\n|---|---|---|\n| No image for 13 |   |   |",
      "metadata": {}
    },
    {
      "id": "86439c50-bd93-4120-af52-8fcbdbbafbd8",
      "cell_type": "markdown",
      "source": "#### GLM Examples\n\n- [1. GLM: Robust Linear Regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-robust.html)\n- [2. GLM-ordinal-features](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-ordinal-features.html)\n- [3. Out-Of-Sample Predictions](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-out-of-sample-predictions.html)\n- [4. Bayesian regression with truncated or censored data](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-truncated-censored-regression.html)\n- [5. Binomial regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-binomial-regression.html)\n- [6. GLM: Negative Binomial Regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-negative-binomial-regression.html)\n- [7. Hierarchical Binomial Model: Rat Tumor Example](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-hierarchical-binomial-model.html)\n- [8. A Primer on Bayesian Methods for Multilevel Modeling](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/multilevel_modeling.html)\n- [9. GLM-missing-values-in-covariates](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-missing-values-in-covariates.html)\n- [10. Regression Models with Ordered Categorical Outcomes](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-ordinal-regression.html)\n- [11. GLM: Poisson Regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-poisson-regression.html)\n- [12. Discrete Choice and Random Utility Models](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-discrete-choice_models.html)\n- [13. GLM: Model Selection](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-model-selection.html)\n- [14. GLM: Robust Regression using Custom Likelihood for Outlier Classification](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-robust-with-outlier-detection.html)\n- [15. Rolling Regression](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-rolling-regression.html)\n\n| 1 | 2 | 3 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/cca03e23e9627b2fb5f4156cb11fa372d2c3f72389827aa6cfb4f0503abc80ab.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/0f445f9b032e3e7693732b2051d3340186c4db3eec9ea8a6d29b2b59b4512af7.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/506205391b7910fb0cf250bf3397ddc842c5bb2f2650e089ebec9d542f49d3fa.png) |\n\n| 4 | 5 | 6 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/60c8fe5e1caad4c617d1a2c0f48e1603df6b2a5f3c0c777e23c2e298ee0ba3b0.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/47b2df508c92dc27eed40a33b0d251d3b893b2509296aaf89312352c4cbf31c5.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/26e650782d73f95310b3fda053366e4f2a9df307cc35e9ed09f3a2fecc1ed06d.png) |\n\n| 7 | 8 | 9 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/02876264b478027ad5229df6fd2030ad6b920fc7092a6c1802da60315485d3f5.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/e9b00f89d8e37f6d896cf10229191d56b5f7d59ff24bcf057d6d2179eaf2408a.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/39b62e02913b56f54c4c218aca75b41ef2510bb5a77d80c74a436d545d569b9b.png) |\n\n| 10 | 11 | 12 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/6928fd059a3b1cdf8ed43348345562fecd7f39a070cfb94d5db76b2345bc99fa.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/8d70edc186902abbd997ed8155b0807406213a2113a01aff27e3e638b83321eb.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/cdb7f44f2cfe84e2cc9a8e023c6db347f18f239b21ab8707d0b0c32520fd2c4f.svg) |\n\n| 13 | 14 | 15 |\n|---|---|---|\n| ![](https://www.pymc.io/projects/examples/en/latest/_images/2abb4b985087a1a8d5f8dfa5b485b34f0134adcceb8cccb3fc9dc0f31ada9047.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/2f5bb1629ac3be1ecc2b35fc544b64a20ee1b7aa236ca42498440805f6ce8fc3.png) | ![](https://www.pymc.io/projects/examples/en/latest/_images/6527831dd88c242e288d444e294f895c0a50eb792a59369a79e32f9290e5c959.png) |\n",
      "metadata": {}
    },
    {
      "id": "1de2aed2-04d4-4a2f-ae42-95f99f62023e",
      "cell_type": "markdown",
      "source": "### Q2: Bayesian Inference using PyMC\n\nBelow are three parts:\n\n1. **Model 1:** Inference for a sample of normal data with a *normal prior* for \\(\\theta\\) and a *gamma prior* for \\(\\tau\\).\n2. **Model 2:** Inference for normal data with a *Laplace prior* for \\(\\theta\\) (non‐normal) and a *LogNormal prior* for \\(\\tau\\) (non‐gamma).\n3. **Model 3:** Inference for normal data with a *Student-t prior* for \\(\\theta\\) (non‐normal) and an *Exponential prior* for \\(\\tau\\) (non‐gamma).\n\nIn each case I provide diagnostic assessments (summary statistics, trace plots, etc.) using ArviZ.\n",
      "metadata": {}
    },
    {
      "id": "4e5984ce-d70c-40a7-b3d5-bfa07c443dc5",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport arviz as az\nfrom scipy import stats\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic normal data\nn = 100\ntrue_theta = 2.0\ntrue_sigma = 1.5\ntrue_tau = 1 / true_sigma**2  # precision\ndata = stats.norm(loc=true_theta, scale=true_sigma).rvs(size=n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'pymc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01marviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maz\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymc'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "c79c8261-bc2a-4d69-b675-93a5ca114475",
      "cell_type": "code",
      "source": "Model 1: Normal prior for theta and Gamma prior for tau\n                                               \nwith pm.Model() as model1:\n    # Priors\n    theta = pm.Normal(\"theta\", mu=0, sigma=10)\n    tau = pm.Gamma(\"tau\", alpha=2, beta=1)\n    \n    # Likelihood\n    x_obs = pm.Normal(\"x_obs\", mu=theta, tau=tau, observed=data)\n    \n    # Sample from the posterior\n    idata1 = pm.sample(draws=2000, tune=1000, chains=4, return_inferencedata=True, target_accept=0.9)\n\n# Diagnostics\nprint(\"Model 1 Summary:\")\nprint(az.summary(idata1, round_to=2))\naz.plot_trace(idata1)\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-3-21d2841e9ff1>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Model 1: Normal prior for theta and Gamma prior for tau\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "1ed4964f-fbe8-44ce-b9ab-5f6c0232e4ca",
      "cell_type": "code",
      "source": "Model 2: Laplace prior for theta and LogNormal prior for tau\n                                                    \nwith pm.Model() as model2:\n    # Priors: Laplace for theta (location=0, scale=5) and LogNormal for tau\n    theta = pm.Laplace(\"theta\", mu=0, b=5)\n    tau = pm.LogNormal(\"tau\", mu=0, sigma=1)\n    \n    # Likelihood\n    x_obs = pm.Normal(\"x_obs\", mu=theta, tau=tau, observed=data)\n    \n    # Sample from the posterior\n    idata2 = pm.sample(draws=2000, tune=1000, chains=4, return_inferencedata=True, target_accept=0.9)\n\n# Diagnostics\nprint(\"Model 2 Summary:\")\nprint(az.summary(idata2, round_to=2))\naz.plot_trace(idata2)\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-4-90c81e1106f1>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Model 2: Laplace prior for theta and LogNormal prior for tau\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "fb031771-a88e-4be6-9ced-7f0c1b59523c",
      "cell_type": "code",
      "source": "Model 3: Student-t prior for theta and Exponential prior for tau\n\nwith pm.Model() as model3:\n    # Priors: Student-t for theta (with 3 degrees of freedom) and Exponential for tau\n    theta = pm.StudentT(\"theta\", nu=3, mu=0, sigma=10)\n    tau = pm.Exponential(\"tau\", lam=1)\n    \n    # Likelihood\n    x_obs = pm.Normal(\"x_obs\", mu=theta, tau=tau, observed=data)\n    \n    # Sample from the posterior\n    idata3 = pm.sample(draws=2000, tune=1000, chains=4, return_inferencedata=True, target_accept=0.9)\n\n# Diagnostics\nprint(\"Model 3 Summary:\")\nprint(az.summary(idata3, round_to=2))\naz.plot_trace(idata3)\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a22d6b01-73b1-4f74-90e8-bc0214774595",
      "cell_type": "markdown",
      "source": "### Q3: Slice Sampling\n\nBelow, I explain how slice sampling works and how it can be incorporated into a Metropolis‐within‐Gibbs algorithm.\n\n**Explanation:**\n\nSlice sampling is a Markov chain Monte Carlo (MCMC) method where, instead of proposing a move and accepting or rejecting it based on an acceptance probability (as in Metropolis–Hastings), I sample uniformly from the region under the graph of the target density. Here’s how it works:\n\n1. **Sampling Below the Density:**  \n   Given the current value \\( x \\) and the target density \\( f(x) \\) (known only up to a normalizing constant), we first draw a vertical level \\( y \\) uniformly from the interval \\([0, f(x)]\\). This effectively \"slices\" the density horizontally.\n\n2. **Finding the Slice:**  \n   The \"slice\" is defined as the set of \\( x \\)-values for which \\( f(x) > y \\). In practice, one finds an interval \\([a, b]\\) containing \\( x \\) where the density exceeds \\( y \\).\n\n3. **Uniform Draw from the Slice:**  \n   A new \\( x' \\) is then drawn uniformly from this interval. If \\( f(x') \\geq y \\), the sample is accepted. If not, the interval is shrunk (based on whether \\( x' \\) lies to the left or right of \\( x \\)) and the process repeats until an acceptable \\( x' \\) is found.\n\n4. **Integration within Gibbs:**  \n   In a Metropolis‐within‐Gibbs algorithm, if full conditional distributions are only known up to a normalizing constant, slice sampling can be used to draw samples from these conditionals. The \"curve\" we sample beneath is the unnormalized full conditional density. The initial value is the current state \\( x \\) and the steps involve:\n   - Drawing a vertical level \\( y \\) uniformly from \\([0, f(x)]\\),\n   - Determining the horizontal slice \\(\\{x: f(x) > y\\}\\),\n   - Sampling a new \\( x' \\) uniformly from this slice.\n\nThis method eliminates the need for calculating an acceptance probability (beyond ensuring the new sample falls under the density) and can adaptively adjust the interval from which samples are drawn.\n\nBelow is a code demonstration of slice sampling:",
      "metadata": {}
    },
    {
      "id": "ff5af6d1-5c53-4d3f-ac77-78e81c98f876",
      "cell_type": "code",
      "source": "Slice Sampling Code Demonstration\n\nfrom scipy import stats\n\ndef slice_f_at_y(f, x, y, x_grid=np.linspace(0, 1, 51)):\n    \"\"\"\n    Find a new sample x' given current value x and level y such that f(x') > y.\n    The function searches for an interval on a provided grid where the density exceeds y,\n    then samples uniformly from the extended interval.\n    \"\"\"\n    # Determine the grid spacing\n    x_grid_delta = x_grid[1] - x_grid[0]\n    # Identify grid points where f(x) > y and extend the interval boundaries slightly\n    valid_points = x_grid[f(x_grid) > y]\n    a, b = valid_points[[0, -1]] + np.array([-x_grid_delta, x_grid_delta])\n    # Sample uniformly within the interval [a, b]\n    x_ = a + stats.uniform().rvs() * (b - a)\n    if f(x_) > y:\n        return x_, 1  # Accepted in one try\n    elif x_ < x:\n        x_l, x_r = x_, b\n    else:\n        x_l, x_r = a, x_\n    return slice_f_at_y_(f, x, y, x_l, x_r, tot=2)\n\ndef slice_f_at_y_(f, x, y, x_l=0, x_r=1, tot=1):\n    \"\"\"\n    Recursive helper function that shrinks the interval [x_l, x_r] until a valid sample is found.\n    \"\"\"\n    x_ = x_l + stats.uniform().rvs() * (x_r - x_l)\n    if f(x_) > y:\n        return x_, tot\n    elif x_ < x:\n        x_l = x_\n    else:\n        x_r = x_\n    return slice_f_at_y_(f, x, y, x_l, x_r, tot=tot+1)\n\n# Define the target density (e.g., a Beta density)\nx_grid = np.linspace(0, 1, 1000)\nf = lambda x: stats.beta(1.5, 3).pdf(x)\n\n# Plot the target density\nplt.figure(figsize=(8, 4))\nplt.plot(x_grid, f(x_grid), label='Beta(1.5, 3) PDF')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Target Density')\nplt.legend()\nplt.show()\n\n# Perform slice sampling for demonstration\nm = 1000\nx_samples = np.zeros(m+1)\n# Initialize the chain at 0.25\nx_samples[0] = 0.25\n\n# For demonstration, we also store the auxiliary y values and iteration counts\ny_values = np.zeros(m)\niter_counts = np.zeros(m, dtype=int)\nplot_trace = 10\n\nfor t in range(m):\n    # Draw vertical level uniformly from [0, f(x)]\n    y = stats.uniform().rvs() * f(x_samples[t])\n    y_values[t] = y\n    # Use slice sampling to propose a new x value\n    x_new, iters = slice_f_at_y(f, x_samples[t], y)\n    iter_counts[t] = iters\n    x_samples[t+1] = x_new\n    # For the first few iterations, plot the horizontal slice and the sample draw\n    if t < plot_trace:\n        plt.plot([x_samples[t]]*2, [0, f(x_samples[t])], 'k-', alpha=0.5)\n        plt.plot([x_samples[t], x_new], [y, y], 'r--', alpha=0.7)\n        \nplt.figure(figsize=(8, 4))\nplt.hist(x_samples, bins=50, density=True, alpha=0.7, label='Slice Sampled x')\nplt.plot(x_grid, f(x_grid), 'k-', lw=2, label='Target PDF')\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title(f'Slice Sampling after {m} iterations')\nplt.legend()\nplt.show()\n\nprint(f\"Average number of iterations per sample: {np.mean(iter_counts):.2f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c0393047-0d12-4fd1-ad6f-a9f5e5d2a7b4",
      "cell_type": "markdown",
      "source": "## Concluding Remarks\n\nIn this notebook, I have:\n\n- Provided a detailed Markdown listing of the PyMC Example Gallery with clickable links and rendered images (Q1).\n- Demonstrated Bayesian inference for normal data using three different prior specifications, along with diagnostic assessments using ArviZ (Q2).\n- Explained the slice sampling algorithm and illustrated how it can be integrated into a Gibbs sampler, complete with a code demonstration (Q3).",
      "metadata": {}
    }
  ]
}